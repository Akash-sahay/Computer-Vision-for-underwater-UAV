{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1jDp1tUV6IgVDSBZ/pFOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akash-sahay/Computer-Vision-for-underwater-UAV/blob/main/Working_with_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du9qJC_A2dri"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab import files\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compensate_color_exponential(image, alpha_R=1, alpha_G=1, alpha_B=1, depth_map=None):\n",
        "    \"\"\"\n",
        "    Compensates for color attenuation in an underwater image.\n",
        "\n",
        "    Parameters:\n",
        "    - image: Input underwater image (in BGR format).\n",
        "    - alpha_R, alpha_G, alpha_B: Attenuation coefficients for Red, Green, and Blue channels respectively.\n",
        "    - depth_map: Estimated depth map of the image (optional). If None, a uniform depth will be assumed.\n",
        "\n",
        "    Returns:\n",
        "    - compensated_image: Color compensated image.\n",
        "    \"\"\"\n",
        "    # Split the image into individual B, G, R channels\n",
        "    B, G, R = cv2.split(image)\n",
        "\n",
        "    # Get image dimensions\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    # If no depth map is provided, assume a uniform depth for simplicity\n",
        "    if depth_map is None:\n",
        "       # Define pink and light blue in RGB\n",
        "        pink = [255, 192, 203]  # RGB values for pink\n",
        "        light_blue = [173, 216, 230]  # RGB values for light blue\n",
        "\n",
        "     # Convert RGB to grayscale-like values to represent depth\n",
        "        pink_depth_value = np.mean(pink) / 255.0  # Normalize to [0, 1] range\n",
        "        blue_depth_value = np.mean(light_blue) / 255.0  # Normalize to [0, 1] range\n",
        "\n",
        "    # Create the depth map\n",
        "        depth_map = np.zeros((height, width))\n",
        "\n",
        "    # Fill upper half with pink depth value\n",
        "        depth_map[:height // 2, :] = pink_depth_value\n",
        "\n",
        "    # Fill lower half with blue depth value\n",
        "        depth_map[height // 2:, :] = blue_depth_value\n",
        "\n",
        "    # Apply the compensation formula for each channel\n",
        "    compensated_R = R * np.exp(-alpha_R * depth_map)\n",
        "    compensated_G = G * np.exp(-alpha_G * depth_map)\n",
        "    compensated_B = B * np.exp(-alpha_B * depth_map)\n",
        "\n",
        "    # Clip the values to the valid range [0, 255] and convert back to uint8\n",
        "    compensated_R = np.clip(compensated_R, 0, 255).astype(np.uint8)\n",
        "    compensated_G = np.clip(compensated_G, 0, 255).astype(np.uint8)\n",
        "    compensated_B = np.clip(compensated_B, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Merge the compensated channels back together\n",
        "    compensated_image = cv2.merge([compensated_B, compensated_G, compensated_R])\n",
        "\n",
        "    return compensated_image"
      ],
      "metadata": {
        "id": "9NRoSqVh3ANl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compensate_color_logarithmic(image, alpha_R=0.5, alpha_G=0.1, alpha_B=0.1, depth_map=None):\n",
        "    \"\"\"\n",
        "    Compensates for color attenuation in an underwater image.\n",
        "\n",
        "    Parameters:\n",
        "    - image: Input underwater image (in BGR format).\n",
        "    - alpha_R, alpha_G, alpha_B: Attenuation coefficients for Red, Green, and Blue channels respectively.\n",
        "    - depth_map: Estimated depth map of the image (optional). If None, a uniform depth will be assumed.\n",
        "\n",
        "    Returns:\n",
        "    - compensated_image: Color compensated image.\n",
        "    \"\"\"\n",
        "    # Split the image into individual B, G, R channels\n",
        "    B, G, R = cv2.split(image)\n",
        "\n",
        "    # Get image dimensions\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "   # If no depth map is provided, assume a uniform depth for simplicity\n",
        "    if depth_map is None:\n",
        "       # Define pink and light blue in RGB\n",
        "        pink = [255, 192, 203]  # RGB values for pink\n",
        "        light_blue = [173, 216, 230]  # RGB values for light blue\n",
        "\n",
        "     # Convert RGB to grayscale-like values to represent depth\n",
        "        pink_depth_value = np.mean(pink) / 255.0  # Normalize to [0, 1] range\n",
        "        blue_depth_value = np.mean(light_blue) / 255.0  # Normalize to [0, 1] range\n",
        "\n",
        "    # Create the depth map\n",
        "        depth_map = np.zeros((height, width))\n",
        "\n",
        "    # Fill upper half with pink depth value\n",
        "        depth_map[:height // 2, :] = pink_depth_value\n",
        "\n",
        "    # Fill lower half with blue depth value\n",
        "        depth_map[height // 2:, :] = blue_depth_value\n",
        "\n",
        "\n",
        "    # Apply the compensation formula for each channel\n",
        "    compensated_R = R * np.log(1-alpha_R*R* depth_map)\n",
        "    compensated_G = G * np.log(1+alpha_G*G* depth_map)\n",
        "    compensated_B = B * np.log(1+alpha_B*B* depth_map)\n",
        "\n",
        "    # Clip the values to the valid range [0, 255] and convert back to uint8\n",
        "    compensated_R = np.clip(compensated_R, 0, 255).astype(np.uint8)\n",
        "    compensated_G = np.clip(compensated_G, 0, 255).astype(np.uint8)\n",
        "    compensated_B = np.clip(compensated_B, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Merge the compensated channels back together\n",
        "    compensated_image = cv2.merge([compensated_B, compensated_G, compensated_R])\n",
        "\n",
        "    return compensated_image"
      ],
      "metadata": {
        "id": "I1lFUebG3IXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def increase_contrast(image):\n",
        "    \"\"\"\n",
        "    Increases the contrast of an image using Histogram Equalization.\n",
        "\n",
        "    Parameters:\n",
        "    - image: Input image (in BGR format).\n",
        "\n",
        "    Returns:\n",
        "    - contrast_image: Contrast-enhanced image.\n",
        "    \"\"\"\n",
        "    # Check if the image is in color\n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "        # Convert to YCrCb color space\n",
        "        ycrcb_image = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
        "\n",
        "        # Apply histogram equalization on the Y channel (luminance)\n",
        "        ycrcb_image[:, :, 0] = cv2.equalizeHist(ycrcb_image[:, :, 0])\n",
        "\n",
        "        # Convert back to BGR color space\n",
        "        contrast_image = cv2.cvtColor(ycrcb_image, cv2.COLOR_YCrCb2BGR)\n",
        "    else:\n",
        "        # If the image is grayscale, apply histogram equalization directly\n",
        "        contrast_image = cv2.equalizeHist(image)\n",
        "\n",
        "    return contrast_image"
      ],
      "metadata": {
        "id": "8TE0zDKQ3YAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def increase_contrast_per_channel(image):\n",
        "    \"\"\"\n",
        "    Increases the contrast of an image by applying histogram equalization to each color channel.\n",
        "\n",
        "    Parameters:\n",
        "    - image: Input image (in BGR format).\n",
        "\n",
        "    Returns:\n",
        "    - contrast_image: Contrast-enhanced image.\n",
        "    \"\"\"\n",
        "    # Split the image into its BGR channels\n",
        "    channels = cv2.split(image)\n",
        "\n",
        "    # Apply histogram equalization to each channel\n",
        "    equalized_channels = [cv2.equalizeHist(channel) for channel in channels]\n",
        "\n",
        "    # Merge the channels back into a single image\n",
        "    contrast_image = cv2.merge(equalized_channels)\n",
        "\n",
        "    return contrast_image\n"
      ],
      "metadata": {
        "id": "bbxQkuB13a7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_frame_Y_contrast(image):\n",
        "  compensated_image_exponential = compensate_color_exponential(image)\n",
        "  compensated_image_rgb_exponential = cv2.cvtColor(compensated_image_exponential, cv2.COLOR_BGR2RGB)\n",
        "  compensated_image_logarithmic = compensate_color_logarithmic(compensated_image_rgb_exponential)\n",
        "  compensated_image_rgb_logarithmic = cv2.cvtColor(compensated_image_logarithmic, cv2.COLOR_BGR2RGB)\n",
        "  contrast_image_after_log = increase_contrast(compensated_image_rgb_logarithmic)\n",
        "  return contrast_image_after_log\n"
      ],
      "metadata": {
        "id": "QMIcgO3W3fK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_frame_contrast_per_channel(image):\n",
        "  contrast_image = increase_contrast_per_channel(image)\n",
        "  return contrast_image"
      ],
      "metadata": {
        "id": "wdbc6dMV-huk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def process_video(input_video_path, output_video_path):\n",
        "    # Open the input video\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "    # Check if the video was opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Cannot open video file {input_video_path}\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Initialize video writer\n",
        "    codec = cv2.VideoWriter_fourcc(*'mp4v')  # Using mp4 codec\n",
        "    out = cv2.VideoWriter(output_video_path, codec, fps, (width, height))\n",
        "\n",
        "    # Check if the video writer was initialized successfully\n",
        "    if not out.isOpened():\n",
        "        print(f\"Error: Cannot write to the output video file {output_video_path}\")\n",
        "        return\n",
        "\n",
        "    # Process each frame\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Apply the final_frame function to each frame\n",
        "        processed_frame = final_frame_contrast_per_channel(frame)\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(processed_frame)\n",
        "\n",
        "        frame_count += 1\n",
        "        if frame_count % 10 == 0:  # Print progress every 10 frames\n",
        "            print(f\"Processed {frame_count} frames...\")\n",
        "\n",
        "    print(f\"Processing completed. Total frames processed: {frame_count}\")\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "# Path to input video and output video\n",
        "input_video_path = '/content/new_pipe_video.mp4'  # Change to your input video path\n",
        "output_video_path = '/content/output_video_new.mp4'\n",
        "\n",
        "# Process the video\n",
        "process_video(input_video_path, output_video_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s18z3xPo4fPv",
        "outputId": "a876f3a8-6794-48e0-bae0-a937af08f994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 10 frames...\n",
            "Processed 20 frames...\n",
            "Processed 30 frames...\n",
            "Processed 40 frames...\n",
            "Processed 50 frames...\n",
            "Processed 60 frames...\n",
            "Processed 70 frames...\n",
            "Processed 80 frames...\n",
            "Processed 90 frames...\n",
            "Processed 100 frames...\n",
            "Processed 110 frames...\n",
            "Processed 120 frames...\n",
            "Processed 130 frames...\n",
            "Processed 140 frames...\n",
            "Processed 150 frames...\n",
            "Processed 160 frames...\n",
            "Processed 170 frames...\n",
            "Processed 180 frames...\n",
            "Processed 190 frames...\n",
            "Processed 200 frames...\n",
            "Processed 210 frames...\n",
            "Processed 220 frames...\n",
            "Processed 230 frames...\n",
            "Processed 240 frames...\n",
            "Processing completed. Total frames processed: 248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUpXuam1-wv4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}